{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ynn431sGLGJ"
      },
      "source": [
        "# 텍스트를 읽고 긍정, 부정 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSNS3jPtGLGM"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# 코드 내부에 한글을 사용가능 하게 해주는 부분입니다.\n",
        "\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,Embedding\n",
        "\n",
        "\n",
        "#주어진 문장을 '단어'로 토큰화 하기\n",
        "\n",
        "#케라스의 텍스트 전처리와 관련한 함수중 text_to_word_sequence 함수를 불러 옵니다.\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3U-_zp6GLGN"
      },
      "source": [
        "# 텍스트의 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LVxPOCEGLGN",
        "outputId": "d23eb18f-a3ca-472d-8333-5679efa14a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "원문:\n",
            " 해보지 않으면 해낼 수 없다\n",
            "\n",
            "토큰화:\n",
            " ['해보지', '않으면', '해낼', '수', '없다']\n",
            "\n",
            "단어 카운트:\n",
            " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화', 3), ('합니다', 1), ('단어로', 1), ('해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('한', 1), ('결과는', 1), ('사용', 1), ('할', 1), ('수', 1), ('있습니다', 1)])\n",
            "\n",
            "문장 카운트:  3\n",
            "\n",
            "각 단어가 몇개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'각': 1, '토큰화': 3, '단어를': 1, '텍스트의': 2, '나누어': 1, '합니다': 1, '먼저': 1, '인식됩니다': 1, '딥러닝에서': 2, '단어로': 1, '해야': 1, '한': 1, '있습니다': 1, '결과는': 1, '수': 1, '사용': 1, '할': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'토큰화': 1, '텍스트의': 2, '딥러닝에서': 3, '먼저': 4, '각': 5, '단어를': 6, '나누어': 7, '합니다': 8, '단어로': 9, '해야': 10, '인식됩니다': 11, '한': 12, '결과는': 13, '사용': 14, '할': 15, '수': 16, '있습니다': 17}\n"
          ]
        }
      ],
      "source": [
        "# 전처리할 텍스트를 정합니다.\n",
        "text = '해보지 않으면 해낼 수 없다'\n",
        " \n",
        "# 해당 텍스트를 토큰화 합니다.\n",
        "result = text_to_word_sequence(text)\n",
        "print(\"\\n원문:\\n\", text)\n",
        "print(\"\\n토큰화:\\n\", result)\n",
        " \n",
        "#단어 빈도수 세기\n",
        "\n",
        "#전처리 하려는 세개의 문장을 정합니다.\n",
        "# 전처리하려는 세 개의 문장을 docs라는 배열에 지정함\n",
        "\n",
        "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n",
        "       '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다.',\n",
        "       '토큰화 한 결과는 딥러닝에서 사용 할 수 있습니다.',\n",
        "       ]\n",
        "# 케라스의 Tokenizer() 함수를 사용하면 단어의 빈도 수를 쉽게 계산할 수 있음\n",
        "# 다음 예제는 위에 제시된 세 문장의 단어를 빈도 수로 다시 정리하는 코드 \n",
        "# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n",
        "token = Tokenizer()             # 토큰화 함수 지정\n",
        "token.fit_on_texts(docs)       # 토큰화 함수에 문장 적용\n",
        " \n",
        "#단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다. \n",
        "#word_counts는 단어의 빈도 수를 계산해주는 함수 \n",
        "print(\"\\n단어 카운트:\\n\", token.word_counts) \n",
        "\n",
        "#Tokenizer()의 word_counts 함수는 순서를 기억하는 OrderedDict클래스를 사용합니다.\n",
        "#순서를 기억하는 OrderedDict 클래스에 담겨 있는 형태로 출력되는 것을 볼 수 있음\n",
        "#document_count( ) 함수를 이용하면 총 몇 개의 문장이 들어있는지를 셀 수도 있음\n",
        "#출력되는 순서는 랜덤입니다. \n",
        "print(\"\\n문장 카운트: \", token.document_count)\n",
        "#word_docs() 함수를 통해 각 단어들이 몇 개의 문장에 나오는가를 세어서 출력할 수도 있음\n",
        "#출력되는 순서는 랜덤임\n",
        "print(\"\\n각 단어가 몇개의 문장에 포함되어 있는가:\\n\", token.word_docs)\n",
        "#각 단어에 매겨진 인덱스 값을 출력하려면 word_index( ) 함수를 사용하면 됨\n",
        "print(\"\\n각 단어에 매겨진 인덱스 값:\\n\",  token.word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myBIkRmFGLGO"
      },
      "source": [
        "# 단어임베딩\n",
        "먼저 짧은 리뷰 10개를 불러와 각각 긍정이면 1이라는 클래스를, 부정적이면 0이라는 클래스로 지정함 <br>\n",
        "케라스에서 제공하는 Tokenizer( ) 함수의 fit_on_text를 이용해 각 단어를 하나의 토큰으로 변환함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ska-bsIyGLGO",
        "outputId": "4d18b5d2-373b-4afa-eec8-571456b07e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
          ]
        }
      ],
      "source": [
        "# 텍스트 리뷰 자료를 지정합니다.\n",
        "docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n",
        "\n",
        "# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정합니다.\n",
        "classes = array([1,1,1,1,1,0,0,0,0,0])\n",
        "\n",
        "# 토큰화 \n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "print(token.word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZmMenRmGLGP"
      },
      "source": [
        "단어가 문장의 다른 요소와 어떤 관계를 가지고 있는지를 알아보는 방법이  필요함 <br>\n",
        "이러한 기법 중에서 가장 기본적인 방법인 원-핫 인코딩(one-hotencoding)을 알아보자 <br>\n",
        "각 단어를 모두 0으로 바꾸어 주고 원하는 단어만 1로 바꾸어 주는 것이 원-핫 인코딩이었음 <br>\n",
        "이를 수행하기 위해 먼저 단어 수만큼 0으로 채워진 벡터 공간으로 바꾸면 다음과 같음 <br>\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH_XzGXPGLGP"
      },
      "source": [
        "이제 토큰에 지정된 인덱스로 새로운 배열을 생성함.\n",
        "그럼 주어진 텍스트는 숫자로 이루어진 다음과 같은 배열로 재편됨.\n",
        "\n",
        "\n",
        "입력된 리뷰 데이터의 토큰 수가 각각 다르다는 것에 유의하시기 바람.\n",
        "딥러닝 모델에 입력을 하려면 학습 데이터의 길이가 동일해야 함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHQ2TYPzGLGP"
      },
      "source": [
        "파이썬 배열의 인덱스는 0부터 시작하므로, 맨 앞에 0이 추가되는 것에 주의함 <br>\n",
        "이제 각 단어가 배열 내에서 해당하는 위치를 1로 바꿔서 벡터화할 수 있음 <br>\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn--AViwGLGP",
        "outputId": "8d0eb244-17bc-4622-b85c-68f01f3bfc3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "리뷰 텍스트, 토큰화 결과:\n",
            " [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"
          ]
        }
      ],
      "source": [
        "x = token.texts_to_sequences(docs)\n",
        "print(\"\\n리뷰 텍스트, 토큰화 결과:\\n\",  x)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "ZOrHf3FJGLGQ"
      },
      "source": [
        "패딩( padding) 과정 :\n",
        "     길이를 똑같이 맞춰 주는 작업\n",
        "패딩은 자연어 처리뿐 아니라 GAN에서도 중요한 역할을 함\n",
        "패딩 작업을 위해 케라스는 pad_sequence( ) 함수를 제공함\n",
        "pad_sequnce( ) 함수를 사용하면 원하는 길이보다 짧은 부분은 숫자 0을 넣어서 채워주고, 긴 데이터는 잘라서 같은 길이로 맞춤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzLhE5b_GLGQ"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37cpWPr-GLGQ",
        "outputId": "7b661dfd-3aeb-40db-cbab-191851d58b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n",
            "\n",
            "리뷰 텍스트, 토큰화 결과:\n",
            " [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n",
            "\n",
            "패딩 결과:\n",
            " [[ 0  0  1  2]\n",
            " [ 0  0  0  3]\n",
            " [ 4  5  6  7]\n",
            " [ 0  8  9 10]\n",
            " [ 0 11 12 13]\n",
            " [ 0  0  0 14]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0 16 17]\n",
            " [ 0  0 18 19]\n",
            " [ 0  0  0 20]]\n",
            "\n",
            "딥러닝 모델 시작:\n"
          ]
        }
      ],
      "source": [
        "# 패딩, 서로 다른 길이의 데이터를 4로 맞추어 줍니다.\n",
        "padded_x = pad_sequences(x, 4)  \n",
        "print(\"\\n패딩 결과:\\n\", padded_x)\n",
        " \n",
        "#딥러닝 모델\n",
        "print(\"\\n딥러닝 모델 시작:\")\n",
        "\n",
        "#임베딩에 입력될 단어의 수를 지정합니다.\n",
        "word_size = len(token.word_index) +1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaCT0ijyGLGQ"
      },
      "source": [
        "원-핫 인코딩을 그대로 사용하면 벡터의 길이가 너무 길어진다는 단점이 있음 <br>\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g21ZpFfzGLGQ"
      },
      "source": [
        "공간적 낭비를 해결하기 위해 등장한 것이 단어 임베딩(word embedding)이라는 방법 <br>\n",
        "단어 임베딩은 주어진 배열을 정해진 길이로 압축시킴 <br>\n",
        "\n",
        "\n",
        "이 과정은 케라스에서 제공하는 Embedding( ) 함수를 사용하면 간단히 해낼 수 있음 <br>\n",
        "임베딩 함수에 필요한 세 가지 파라미터는 ‘입력, 출력, 단어 수’임 <br>\n",
        "총 몇 개의 단어 집합에서(입력), 몇 개의 임베딩 결과를 사용할 것인지(출력), 그리고 매번 입력될 단어 수는 몇 개로 할지(단어 수)를 정해야 하는 것\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_fwjVINGLGQ"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-xHOcgfGLGQ"
      },
      "source": [
        "몇 개의 임베딩 결과를 사용할 것인지, 즉 ‘출력’을 정할 차례임\n",
        "word_size만큼의 입력 값을 이용해 8개의 임베딩 결과를 만듦\n",
        "여기서 8이라는 숫자는 임의로 정한 것\n",
        "데이터에 따라 적절한 값으로 바꿀 수 있음\n",
        "이때 만들어진 8개의 임베딩 결과는 우리 눈에 보이지 않음\n",
        "내부에서 계산하여 딥러닝의 레이어로 활용됨\n",
        "끝으로 매번 입력될 ‘단어 수’를 정함\n",
        "패딩 과정을 거쳐 4개의 길이로 맞춰 주었으므로 4개의 단어가 들어가게 설정하면 임베딩 과정은 다음 한 줄로 표현됨\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W881VwygGLGQ"
      },
      "outputs": [],
      "source": [
        "#단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력합니다.\n",
        "model = Sequential()\n",
        "model.add(Embedding(word_size, 8, input_length=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EWEnt6_GLGR",
        "outputId": "74e0d536-c9d3-4801-fe71-15ca39398488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6925 - accuracy: 0.6000\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6905 - accuracy: 0.6000\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6886 - accuracy: 0.6000\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6867 - accuracy: 0.7000\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6848 - accuracy: 0.7000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6829 - accuracy: 0.8000\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6809 - accuracy: 0.8000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6790 - accuracy: 0.8000\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6771 - accuracy: 0.8000\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6752 - accuracy: 0.8000\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6733 - accuracy: 0.8000\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6714 - accuracy: 0.9000\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6695 - accuracy: 0.9000\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6676 - accuracy: 0.9000\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6656 - accuracy: 0.9000\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.9000\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6617 - accuracy: 0.9000\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6598 - accuracy: 0.9000\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6578 - accuracy: 0.9000\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6559 - accuracy: 0.9000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6539 - accuracy: 0.9000\n",
            "\n",
            " Accuracy: 0.9000\n"
          ]
        }
      ],
      "source": [
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(padded_x, classes, epochs=20)\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_x, classes)[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIbYvxScGLGR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}